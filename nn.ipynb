{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch  # Main PyTorch library\n",
    "import torch.nn as nn  # Neural networks module in PyTorch\n",
    "import torch.optim as optim  # Optimization algorithms module in PyTorch\n",
    "import torch.nn.functional as F  # Functional API that contains utility functions like activation functions\n",
    "from torch.utils.data import DataLoader  # DataLoader for easy data loading and batching\n",
    "import torchvision.datasets as datasets  # Datasets module for common datasets\n",
    "import torchvision.transforms as transforms  # Transforms module for common image transformations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Fully Connected Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "class NN(nn.Module):                                # Define a neural network class named NN, inheriting from nn.Module\n",
    "    def __init__(self, input_size, num_classes):    # 784 (nodes) input size (28*28 images)\n",
    "        super(NN, self).__init__()                  # Call the constructor of the parent class (nn.Module)\n",
    "        self.fc1 = nn.Linear(input_size, 50)        # Define the first fully connected layer with input_size nodes and 50 output nodes   \n",
    "        self.fc2 = nn.Linear(50, num_classes)       # Define the second fully connected layer with 50 input nodes and num_classes output nodes\n",
    "\n",
    "   \n",
    "    def forward(self, x):           # Define the forward pass of the neural network\n",
    "        x = F.relu(self.fc1(x))     # Apply Rectified Linear Unit (ReLU) activation to the output of the first fully connected layer\n",
    "        x = self.fc2(x)             # Pass the result through the second fully connected layer               \n",
    "        return x                    # Return the final output\n",
    "    \n",
    "model = NN(784, 10)         # Instantiate the neural network model with input size 784 and 10 output classes\n",
    "x = torch.randn(64, 784)    # Create a random input tensor with shape (64, 784)\n",
    "print(model(x).shape)       # Print the shape of the output produced by the model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU ('cuda') if available, otherwise use CPU ('cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for the neural network training\n",
    "input_size = 784            # Number of input features (e.g., flattened size of images)\n",
    "num_classes = 10            # Number of output classes\n",
    "learning_rate = 0.001       # Learning rate for the optimization algorithm\n",
    "batch_size = 64             # Number of input samples in each mini-batch\n",
    "num_epochs = 5              # Number of complete passes through the entire training dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Custom dataset class for the CSV data\n",
    "class CustomMNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data_frame.iloc[idx, 1:-1].values.astype('float32').reshape(28, 28)\n",
    "        label = self.data_frame.iloc[idx, -1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define transformations, if any (e.g., ToTensor)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load training and testing data from CSV files\n",
    "train_dataset = CustomMNISTDataset(csv_file='train.csv', transform=transform)\n",
    "test_dataset = CustomMNISTDataset(csv_file='test.csv', transform=transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "batch_size = 64  # Example batch size, adjust as needed\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)  # Typically no need to shuffle test data\n",
    "\n",
    "# The train_loader and test_loader can now be used in a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create training and testing datasets using MNIST, applying transformations and downloading if necessary\n",
    "# train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "# test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# # Create DataLoader instances for training and testing datasets, specifying batch size and enabling shuffling\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the neural network model with specified input size and number of classes, and move it to the specified device (GPU or CPU)\n",
    "model = NN(input_size=input_size, num_classes=num_classes).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss criterion as CrossEntropyLoss and the optimization algorithm as Adam\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop iterating over epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Iterate over batches in the training DataLoader\n",
    "    for data, targets in train_loader:\n",
    "        # Move data and targets to the specified device (GPU or CPU)\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "        # Reshape data to the correct shape (flattening for fully connected layers)\n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Backward pass and optimization step\n",
    "        optimizer.zero_grad()  # Clear gradients from previous backward passes\n",
    "        loss.backward()       # Backpropagation to compute gradients\n",
    "        optimizer.step()       # Update model parameters using the optimizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check accuracy on training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check the accuracy of the model on a given DataLoader\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches in the DataLoader\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            \n",
    "            # Forward pass to obtain predictions\n",
    "            scores = model(x)\n",
    "            \n",
    "            # Find the index of the maximum score (predicted class) for each sample\n",
    "            _, predictions = scores.max(1)\n",
    "            \n",
    "            # Count the number of correct predictions\n",
    "            num_correct += (predictions == y).sum().item()\n",
    "            \n",
    "            # Update the total number of samples processed\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        # Calculate and print accuracy\n",
    "        accuracy = float(num_correct) / float(num_samples) * 100\n",
    "        print(f'Got {num_correct}/{num_samples} with accuracy {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 56967/60000 with accuracy 94.95\n",
      "Got 9462/10000 with accuracy 94.62\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(train_loader,model)\n",
    "# check_accuracy(test_loader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
